{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lj-yOVzjIYP3"
   },
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weAE0BBqIeG6"
   },
   "source": [
    "you can manually download the dataset with link : https://drive.google.com/file/d/1wYVdCSW8ipjJ2Zr8zoqe1w1LGc5jhufW/view?usp=sharing\n",
    "\n",
    "and insert the dataset on working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjQpPclRG6MT",
    "outputId": "cbb55d07-19bc-4d21-ae02-c28370f420ab"
   },
   "outputs": [],
   "source": [
    "# Download the dataset from MEGA will takes 15-20 Minutes on Google Colab\n",
    "!pip install mega.py\n",
    "\n",
    "from mega import Mega\n",
    "\n",
    "mega = Mega()\n",
    "\n",
    "m = mega.login()\n",
    "\n",
    "m.download_url('https://mega.nz/file/bBoSSRra#szRg4E4glGBVUryt5-BI5GvaRymLYPYsIMxbH9Soflc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHYW75pv0PQO"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "local_zip = 'Dataset.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9PHumnYGHb1"
   },
   "source": [
    "## Import Required Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7Li9Zma0dGo",
    "outputId": "5dce13a6-2fc8-4caf-8b95-3b0a3c1db764"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(101)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "print(\"Running TensorflowVersion: \" + str(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C41l7_iyGMja"
   },
   "source": [
    "## Select appropriate distribution strategy for hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0ZOFQKY09jR",
    "outputId": "bb7dcd24-8b73-40c5-99a3-ee8bb7ad1e6e"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "    gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    print(\"Running on TPU: \" + str(tpu.master()))\n",
    "elif len(gpus) > 0:\n",
    "    strategy = tf.distribute.MirroredStrategy(gpus)\n",
    "    print(\"Running on \",len(gpus),\" GPU(s)\")\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AU0V1TGdGSb5"
   },
   "source": [
    "## Setup Strategy Based on Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QbNrNAhn1CkI",
    "outputId": "e8819e25-d2c3-4a3f-dedc-e13723f00cc9"
   },
   "outputs": [],
   "source": [
    "train_path = \"Dataset/Train\"\n",
    "val_path = \"Dataset/Validation\"\n",
    "test_path = \"Dataset/Test\"\n",
    "\n",
    "train_images = [image for dir,_,sublist in os.walk(train_path) for image in sublist]\n",
    "val_images = [image for dir,_,sublist in os.walk(val_path) for image in sublist]\n",
    "test_images = [image for dir,_,sublist in os.walk(test_path) for image in sublist]\n",
    "num_train_images = len(train_images)\n",
    "num_val_images = len(val_images)\n",
    "num_test_images = len(test_images)\n",
    "\n",
    "IMAGE_SIZE = 224 \n",
    "EPOCHS = 10\n",
    "\n",
    "#CLASSES = ['Boat', 'Bus', 'Car', 'Cat', 'Flower', 'Horse']\n",
    "multi = 16 \n",
    "\n",
    "#Learning rate scheduling variables\n",
    "num_units = strategy.num_replicas_in_sync\n",
    "if num_units == 8:\n",
    "    BATCH_SIZE = multi * num_units\n",
    "    VALIDATION_BATCH_SIZE = multi * num_units\n",
    "    start_lr = 0.00001\n",
    "    min_lr = 0.00001\n",
    "    max_lr = 0.00005 * num_units\n",
    "    rampup_epochs = 8\n",
    "    sustain_epochs = 0\n",
    "    exp_decay = 0.8\n",
    "elif num_units == 1:\n",
    "    BATCH_SIZE = multi\n",
    "    VALIDATION_BATCH_SIZE = multi\n",
    "    start_lr = 0.00001 #0.00001\n",
    "    min_lr = 0.00001 #0.00001\n",
    "    max_lr = 0.0002 #0.0002\n",
    "    rampup_epochs = 8 #8\n",
    "    sustain_epochs = 0\n",
    "    exp_decay = 0.8\n",
    "else:\n",
    "    BATCH_SIZE = int(multi/2) * num_units\n",
    "    VALIDATION_BATCH_SIZE = int(multi/2) * num_units\n",
    "    start_lr = 0.00001  \n",
    "    min_lr = 0.00001\n",
    "    max_lr = 0.00002 * num_units\n",
    "    rampup_epochs = 11\n",
    "    sustain_epochs = 0\n",
    "    exp_decay = 0.8\n",
    "    \n",
    "train_steps = int(np.ceil(num_train_images/BATCH_SIZE))\n",
    "val_steps = int(np.ceil(num_val_images/VALIDATION_BATCH_SIZE))\n",
    "\n",
    "print(\"Total Training Images: \" + str(num_train_images))\n",
    "print(\"Total Validation Images: \" + str(num_val_images))\n",
    "print(\"Total Test Images: \" + str(num_test_images))\n",
    "\n",
    "print(\"Train Steps: \" + str(train_steps))\n",
    "print(\"Val steps: \" + str(val_steps))\n",
    "\n",
    "def display_training_curves(training,validation,title,subplot):\n",
    "    if subplot%10 == 1:\n",
    "        plt.subplots(figsize = (10,10),facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    ax = plt.subplot(subplot)\n",
    "    ax.set_facecolor('#F8F8F8')\n",
    "    ax.plot(training)\n",
    "    ax.plot(validation)\n",
    "    ax.set_title('model '+ title)\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'valid.'])\n",
    "    \n",
    "def learningrate_function(epoch):\n",
    "    if epoch < rampup_epochs:\n",
    "        lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
    "    elif epoch < rampup_epochs + sustain_epochs:\n",
    "        lr = max_lr\n",
    "    else:\n",
    "        lr = (max_lr - min_lr) * exp_decay**(epoch - rampup_epochs - sustain_epochs) + min_lr\n",
    "    return lr\n",
    "\n",
    "def learning_rate_callback():\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch : learningrate_function(epoch),verbose = True)\n",
    "    rng = [i for i in range(EPOCHS)]\n",
    "    y = [learningrate_function(x) for x in range(EPOCHS)]\n",
    "    plt.plot(rng,y)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UF0gB5nKGVnI"
   },
   "source": [
    "## Define Training, Validation, Test Dataset through Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dXRswAD1Gqm",
    "outputId": "1b582d4f-324c-43e1-a41a-665fa71ae0c0"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "train_datagen = datagen.flow_from_directory(train_path, \n",
    "                                            target_size = (IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                            batch_size = BATCH_SIZE, \n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "val_datagen = datagen.flow_from_directory(val_path,\n",
    "                                          target_size = (IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                          batch_size = VALIDATION_BATCH_SIZE,\n",
    "                                          class_mode = 'categorical')\n",
    "\n",
    "test_datagen = datagen.flow_from_directory(test_path,\n",
    "                                            target_size = (IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                            batch_size = 1, \n",
    "                                            class_mode = 'categorical',\n",
    "                                            shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADZroiBL1JYO",
    "outputId": "d15cfbb8-0fa8-481b-8e55-056a4af0ccc3"
   },
   "outputs": [],
   "source": [
    "CLASSES = list(train_datagen.class_indices.keys())\n",
    "print(CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6954YC4bGbdL"
   },
   "source": [
    "## Define Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 789
    },
    "id": "CwkHqMKW1Ln6",
    "outputId": "6e295ebe-3616-43e6-dc1f-985bdb921c48"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():        \n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False) \n",
    "    base_model.trainable = True\n",
    "    \n",
    "    model = Sequential([\n",
    "        base_model,       \n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        Dense(6, activation = 'softmax', dtype= tf.float32)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "lr_callback = learning_rate_callback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfDr-ms5GiY-"
   },
   "source": [
    "## Train the model (you can skip this step if only load the existing model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "us-Ke8sXO4cL"
   },
   "source": [
    "you can manually download existing model (the exact model are used on the project) and insert on working directory with following link : https://drive.google.com/file/d/188gdv5nLEQfFAnCqHSmq9bX6FRkh34E_/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6LD87EA1OI9",
    "outputId": "e7fd2a44-e90a-4508-ff11-bdc66407898f"
   },
   "outputs": [],
   "source": [
    "filepath = \"model.h5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max') #,mode='max'\n",
    "\n",
    "callbacks_list = [checkpoint,lr_callback]\n",
    "\n",
    "hist = model.fit(train_datagen,\n",
    "                             steps_per_epoch = train_steps,\n",
    "                             validation_data = val_datagen,\n",
    "                             validation_steps = val_steps,\n",
    "                             epochs = EPOCHS,\n",
    "                             verbose = 1,\n",
    "                             callbacks = callbacks_list)\n",
    "\n",
    "\n",
    "\n",
    "val_loss, val_acc = model.evaluate(val_datagen,steps = num_val_images)\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_acc:', val_acc)\n",
    "\n",
    "display_training_curves(hist.history['loss'], hist.history['val_loss'], 'loss', 211) \n",
    "display_training_curves(hist.history['accuracy'], hist.history['val_accuracy'], 'accuracy', 212)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLcyD0upGpXs"
   },
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzYbQlnoGqbs"
   },
   "outputs": [],
   "source": [
    "model = load_model(\"model.h5\")\n",
    "model.summary()\n",
    "\n",
    "val_loss, val_acc = model.evaluate(val_datagen,steps = num_val_images)\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_acc:', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9-VRxEZGv9y"
   },
   "source": [
    "## Test the accuracy using test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5tYhVG-Gx9t"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_datagen, steps= num_val_images, verbose=1)\n",
    "\n",
    "df_preds = pd.DataFrame(predictions,columns=CLASSES)\n",
    "#df_preds.head()\n",
    "y_true = val_datagen.classes\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate a classification report\n",
    "\n",
    "# For this to work we need y_pred as binary labels not as probabilities\n",
    "y_pred_binary = predictions.argmax(axis=1)\n",
    "\n",
    "report = classification_report(y_true, y_pred_binary, target_names=CLASSES)\n",
    "\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Create Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
